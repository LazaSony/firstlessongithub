{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Problem:\nHumans are very sensitive to humidity, as the skin relies on the air to get rid of moisture. The process of sweating is your body's attempt to keep cool and maintain its current temperature. If the air is at 100-percent relative humidity, sweat will not evaporate into the air. As a result, we feel much hotter than the actual temperature when the relative humidity is high. If the relative humidity is low, we can feel much cooler than the actual temperature because our sweat evaporates easily, cooling us off. For example, if the air temperature e is 24 degrees Celsius and the relative humidity is zero percent, the air temperature feels like 21 C to our bodies. If the air temperature is 24 C and the relative humidity is 100 percent, we feel like it's 80 degrees (27 C) out.\n\n\nCreate a model that will predi Relative Humidity from other variables.\n\n---\n\nData description:\n\n0. Date (DD/MM/YYYY)\n1. Time (HH.MM.SS)\n2. True hourly averaged concentration CO in mg/m^3 (reference analyzer)\n3. PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)\n4. True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer)\n5. True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)\n6. PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)\n7. True hourly averaged NOx concentration in ppb (reference analyzer)\n8. PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)\n9. True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)\n10. PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)\n11. PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)\n12. Temperature in Â°C\n13. Relative Humidity (%)\n14. AH Absolute Humidity\n\n---\n\nLoad data, sort out missing data (fill it or remove columns/rows)\n\nIn order for ML model to be able to work with Date, convert month (int) variable. Remove original Date after.\n\nSimilarly, convert Time's hour to (int) variable. Drop original Time after.\n\nAfter all of that is done and data is ready. Create a baseline using `LinearRegression()`. No preprocessing needed, no optuna, just a simple baseline to see if it is working. For evaluation use `mean_absolute_error()`.\n\n---\n\nOnce this is working, take the Optuna code you made during last lesson and adjust it to work with regression.\n\n0. Change function where it loads the data to what you have created earlier.\n1. You will need to change part where it chooses `classifiers` into `regressors`. Use folllowing: `LinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, `SVR`. Look these up at sci-kit and add some parameters you think might be impactful for Optuna to optimize.\n2. You will need to change `stratifiedKfoldSplit` - which work with classification exapmles only, to a cross-validation method that can work with regression. ([here are](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#visualize-cross-validation-indices-for-many-cv-objects) the plots if you need help choosing)\n3. change evaluation metric into whatever you choose to use during the baseline. Make sure optuna is set to optimize in the right direction!\n4. Apart from the existing numerical-data-scalers optuna chooses from, add `Normalizer` to the choice. Find it in Scikit, import, and add it.\n\nIn the end, Optuna should be able to run 100 trials.",
   "metadata": {
    "tags": [],
    "cell_id": "00000-b841696f-f4e8-4266-8297-86f427b4b34f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-a1fbde8c-6a16-47d1-bd25-c862e5304334",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7a831acc",
    "execution_start": 1633283279287,
    "execution_millis": 21468,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install -q xlrd\n!pip install -q optuna\n!pip install -q openpyxl\n\nprint(\"-------------- Necessary packages installed --------------\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, StandardScaler, PowerTransformer,Normalizer\nimport pandas as pd\nimport datetime as dt\nimport optuna\n\n\nprint(\"-------------- Packages loaded --------------\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "-------------- Necessary packages installed --------------\n-------------- Packages loaded --------------\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-ad5c6457-0f68-47b1-8128-f5c382bdafa6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c630fe5e",
    "execution_start": 1633283303978,
    "execution_millis": 4860,
    "deepnote_cell_type": "code"
   },
   "source": "data_path = \"/work/data/homework 22/AirQualityUCI.xlsx\"\ncol_names = ['date', 'time', 'co_gt', 'pt08_s1_co', 'nmhc_gt', 'c6h6_gt', 'pt08_s2_nmhc', 'nox_gt', 'pt08_s3_nox', 'no2_gt', 'pt08_s4_no2', 'pt08_s5_o3', 't', 'rh', 'ah']\n\ndf = pd.read_excel(data_path, names=col_names, usecols=range(15)) # usecols parameters is there because for some reason read_excel reads the file with 2 more (completely empty) columns than there actually are, so I am telling it to only load first 15 cols - there are 15 cols in the datasets \ndf[\"date\"] = df[\"date\"].astype(int)/ 10**9\ndf[\"time\"] = df[\"time\"].astype(str).str.split(\":\").apply(lambda x: int(x[0]) * 60 + int(x[1]))\nX = df.drop([\"rh\"],axis=1)\ny= df[\"rh\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3,random_state=42,shuffle=True)\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\nmodel=LinearRegression()\nscaled_X_train = scaler.transform(X_train)\nscaled_X_test = scaler.transform(X_test)\nmodel.fit(scaled_X_train,y_train)\nprediction = model.predict(scaled_X_test)\nprint('MAE:', metrics.mean_absolute_error(y_test, prediction))\n\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "MAE: 5.697231904290597\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-1253bf2f-4056-4c10-bd03-7bf258735680",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f27ce3f9",
    "execution_start": 1633283338406,
    "execution_millis": 91564,
    "deepnote_cell_type": "code"
   },
   "source": "def load_data():\n    data_path = \"/work/data/homework 22/AirQualityUCI.xlsx\"\n    col_names = ['date', 'time', 'co_gt', 'pt08_s1_co', 'nmhc_gt', 'c6h6_gt', 'pt08_s2_nmhc', 'nox_gt', 'pt08_s3_nox', 'no2_gt', 'pt08_s4_no2', 'pt08_s5_o3', 't', 'rh', 'ah']\n    df = pd.read_excel(data_path, names=col_names, usecols=range(15))\n    df[\"date\"] = df[\"date\"].astype(int)/ 10**9\n    df[\"time\"] = df[\"time\"].astype(str).str.split(\":\").apply(lambda x: int(x[0]) * 60 + int(x[1]))\n    X = df.drop([\"rh\"],axis=1)\n    y= df[\"rh\"]\n    return X, y\n\n\ndef objective(trial):\n\n    \n    regressor_name = trial.suggest_categorical(\"regressors\", [\"SVR\", \"RandomForestRegressor\",\"DecisionTreeRegressor\",\"LinearRegression\"])\n    if regressor_name == \"SVR\":\n        svr_c = trial.suggest_float(\"svr_c\", 1e-3, 1e3, log=True)\n        model = SVR(C=svr_c)\n    elif regressor_name == \"RandomForestRegressor\":\n        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 12, log=True)\n        model = RandomForestRegressor(max_depth=rf_max_depth, \n                                                       n_estimators=10)\n    elif regressor_name == \"DecisionTreeRegressor\":\n        dt_criteria = trial.suggest_categorical('criterion', ['mse', 'friedman_mse'])\n        dt_max_depth = trial.suggest_int(\"dt_max_depth\", 2, 12)\n        model = DecisionTreeRegressor(criterion= dt_criteria,\n                                                   max_depth=dt_max_depth)\n    elif regressor_name == 'LinearRegression':\n        model = LinearRegression() \n    \n\n    scaler_string = trial.suggest_categorical(\"------------------------------------_scaler\",[\"no_scaler\", \"StandardScaler\",\"RobustScaler\",\"MinMaxScaler\", \"MaxAbsScaler\", \"StandardScaler\", \"PowerTransformer\",\"Normalizer\"])    \n    if scaler_string == \"no_scaler\":\n        scaled_X = X\n    else:\n        scaler = eval(scaler_string)()\n        scaler.fit(X)\n        scaled_X = scaler.transform(X)\n\n    cv=KFold(shuffle=True,random_state=42)\n    score=cross_val_score(model,scaled_X,y,cv=cv,scoring=\"neg_mean_absolute_error\")\n    trial_score = score.mean() \n\n    return trial_score\n\nX,y = load_data()\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=10)\nprint(study.best_trial)\n\n\n        ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001b[32m[I 2021-10-03 17:49:02,845]\u001b[0m A new study created in memory with name: no-name-d693a011-1c39-41a8-a8e0-0280d11ff3aa\u001b[0m\n\u001b[32m[I 2021-10-03 17:49:03,679]\u001b[0m Trial 0 finished with value: -11.310037343770166 and parameters: {'regressors': 'RandomForestRegressor', 'rf_max_depth': 2, '------------------------------------_scaler': 'RobustScaler'}. Best is trial 0 with value: -11.310037343770166.\u001b[0m\n\u001b[32m[I 2021-10-03 17:49:31,662]\u001b[0m Trial 1 finished with value: -10.849885498063445 and parameters: {'regressors': 'SVR', 'svr_c': 0.8296090641500931, '------------------------------------_scaler': 'StandardScaler'}. Best is trial 1 with value: -10.849885498063445.\u001b[0m\n\u001b[32m[I 2021-10-03 17:49:31,756]\u001b[0m Trial 2 finished with value: -5.713765012476422 and parameters: {'regressors': 'LinearRegression', '------------------------------------_scaler': 'MinMaxScaler'}. Best is trial 2 with value: -5.713765012476422.\u001b[0m\n\u001b[32m[I 2021-10-03 17:49:32,042]\u001b[0m Trial 3 finished with value: -13.8240578455938 and parameters: {'regressors': 'DecisionTreeRegressor', 'criterion': 'mse', 'dt_max_depth': 3, '------------------------------------_scaler': 'Normalizer'}. Best is trial 2 with value: -5.713765012476422.\u001b[0m\n\u001b[32m[I 2021-10-03 17:49:32,317]\u001b[0m Trial 4 finished with value: -13.782961180173189 and parameters: {'regressors': 'DecisionTreeRegressor', 'criterion': 'mse', 'dt_max_depth': 6, '------------------------------------_scaler': 'Normalizer'}. Best is trial 2 with value: -5.713765012476422.\u001b[0m\n\u001b[32m[I 2021-10-03 17:50:00,537]\u001b[0m Trial 5 finished with value: -4.728331724218805 and parameters: {'regressors': 'SVR', 'svr_c': 6.085035323066459, '------------------------------------_scaler': 'StandardScaler'}. Best is trial 5 with value: -4.728331724218805.\u001b[0m\n\u001b[32m[I 2021-10-03 17:50:27,545]\u001b[0m Trial 6 finished with value: -23.374796073903 and parameters: {'regressors': 'SVR', 'svr_c': 0.004857799826960628, '------------------------------------_scaler': 'MaxAbsScaler'}. Best is trial 5 with value: -4.728331724218805.\u001b[0m\n\u001b[32m[I 2021-10-03 17:50:27,685]\u001b[0m Trial 7 finished with value: -5.713765012476422 and parameters: {'regressors': 'LinearRegression', '------------------------------------_scaler': 'MinMaxScaler'}. Best is trial 5 with value: -4.728331724218805.\u001b[0m\n\u001b[32m[I 2021-10-03 17:50:29,796]\u001b[0m Trial 8 finished with value: -4.047472023310563 and parameters: {'regressors': 'RandomForestRegressor', 'rf_max_depth': 6, '------------------------------------_scaler': 'MinMaxScaler'}. Best is trial 8 with value: -4.047472023310563.\u001b[0m\n\u001b[32m[I 2021-10-03 17:50:29,951]\u001b[0m Trial 9 finished with value: -6.224130870959838 and parameters: {'regressors': 'LinearRegression', '------------------------------------_scaler': 'Normalizer'}. Best is trial 8 with value: -4.047472023310563.\u001b[0m\nFrozenTrial(number=8, values=[-4.047472023310563], datetime_start=datetime.datetime(2021, 10, 3, 17, 50, 27, 740276), datetime_complete=datetime.datetime(2021, 10, 3, 17, 50, 29, 795852), params={'regressors': 'RandomForestRegressor', 'rf_max_depth': 6, '------------------------------------_scaler': 'MinMaxScaler'}, distributions={'regressors': CategoricalDistribution(choices=('SVR', 'RandomForestRegressor', 'DecisionTreeRegressor', 'LinearRegression')), 'rf_max_depth': IntLogUniformDistribution(high=12, low=2, step=1), '------------------------------------_scaler': CategoricalDistribution(choices=('no_scaler', 'StandardScaler', 'RobustScaler', 'MinMaxScaler', 'MaxAbsScaler', 'StandardScaler', 'PowerTransformer', 'Normalizer'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=8, state=TrialState.COMPLETE, value=None)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7d3ce7c8-a514-49e4-9ba4-a5899ac52ea5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "3100ade8-6caf-4298-97e3-2ed97e06889b",
  "deepnote_execution_queue": []
 }
}